{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #FFC300\">VI. Model Building</h3>\n",
    "<p style=\"font-family: Comic Sans MS\">Now I evaluate the effectiveness of machine learning algorithms in solving this binary classification problem using the selected features. The algorithms used include Decision Tree, Random Forest, Logistic Regression, K-Nearest Neighbors, Naive Bayes, Support Vector Machine, AdaBoost, and XGBoost. I use first the default parameters and then for fine-tunning the algorithms I create Grid Search for each algorithm to find the best hyperparameter values. Then I use the following metrics for evaluation: accuracy, precision, recall and F1-Score.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_final = pd.read_csv('../data/processed/dga_features_final.csv')\n",
    "\n",
    "key_features = ['long_consonant_str', 'unique_char_count', 'entropy', 'vowel_ratio',\n",
    "                'unique_letter_count', 'd_length', 'consonant_ratio', 'unique_digit_count', 'ngrams']\n",
    "\n",
    "X = df_final[key_features]\n",
    "y = df_final['isDGA']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entire dataset has 159995 rows and 19 columns.\n",
      "The training set has 127996 rows and 9 columns.\n",
      "The testing set has 31999 rows and 9 columns.\n",
      "The target training set has 127996 rows.\n",
      "The target testing set has 31999 rows.\n"
     ]
    }
   ],
   "source": [
    "# Dimensions of the training and testing sets\n",
    "print(f\"The entire dataset has {df_final.shape[0]} rows and {df_final.shape[1]} columns.\")\n",
    "print(f\"The training set has {X_train.shape[0]} rows and {X_train.shape[1]} columns.\")\n",
    "print(f\"The testing set has {X_test.shape[0]} rows and {X_test.shape[1]} columns.\")\n",
    "print(f\"The target training set has {y_train.shape[0]} rows.\")\n",
    "print(f\"The target testing set has {y_test.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #FFC300\">A. Test with Decision Tree Classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hyperparams = {'criterion': ['gini', 'entropy'],\n",
    "               'max_depth': [None, 5, 10, 15, 20],\n",
    "               'min_samples_split': [2, 5, 10, 15, 20],\n",
    "               'min_samples_leaf': [1, 2, 5, 10, 15]}\n",
    "\n",
    "df_clf_gs = GridSearchCV(dt_clf, hyperparams, cv=5)\n",
    "df_clf_gs.fit(X_train, y_train)\n",
    "print(f\"Best hyperparameters: {df_clf_gs.best_params_}\")\n",
    "print(f\"Best score: {df_clf_gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dt_clf_opt = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf=5, min_samples_split=15, random_state=42)\n",
    "dt_clf_opt.fit(X_train, y_train)\n",
    "y_pred_opt = dt_clf_opt.predict(X_test)\n",
    "\n",
    "accuracy_opt = accuracy_score(y_test, y_pred_opt)\n",
    "precision_opt = precision_score(y_test, y_pred_opt)\n",
    "recall_opt = recall_score(y_test, y_pred_opt)\n",
    "f1_opt = f1_score(y_test, y_pred_opt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_opt}\")\n",
    "print(f\"Precision: {precision_opt}\")\n",
    "print(f\"Recall: {recall_opt}\")\n",
    "print(f\"F1 Score: {f1_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(dt_clf_opt, open('../models/dt_clf_opt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #FFC300\">B. Test with Random Forest Classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_rf}\")\n",
    "print(f\"Precision: {precision_rf}\")\n",
    "print(f\"Recall: {recall_rf}\")\n",
    "print(f\"F1 Score: {f1_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hyperparams_rf = {'n_estimators': [50, 100, 150, 200],\n",
    "                    'criterion': ['gini', 'entropy'],\n",
    "                    'max_depth': [None, 5, 10, 15, 20],\n",
    "                    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "                    'min_samples_leaf': [1, 2, 5, 10, 15]}\n",
    "\n",
    "rf_gs = GridSearchCV(rf, hyperparams_rf, cv=5)\n",
    "rf_gs.fit(X_train, y_train)\n",
    "print(f\"Best hyperparameters: {rf_gs.best_params_}\")\n",
    "print(f\"Best score: {rf_gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_opt = RandomForestClassifier(n_estimators=150, criterion='entropy', max_depth=20, min_samples_leaf=1, min_samples_split=2, random_state=42)\n",
    "rf_opt.fit(X_train, y_train)\n",
    "y_pred_rf_opt = rf_opt.predict(X_test)\n",
    "\n",
    "accuracy_rf_opt = accuracy_score(y_test, y_pred_rf_opt)\n",
    "precision_rf_opt = precision_score(y_test, y_pred_rf_opt)\n",
    "recall_rf_opt = recall_score(y_test, y_pred_rf_opt)\n",
    "f1_rf_opt = f1_score(y_test, y_pred_rf_opt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_rf_opt}\")\n",
    "print(f\"Precision: {precision_rf_opt}\")\n",
    "print(f\"Recall: {recall_rf_opt}\")\n",
    "print(f\"F1 Score: {f1_rf_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(rf_opt, open('../models/rf_opt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #FFC300\">C. Test with Logistic Regression</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr)\n",
    "recall_lr = recall_score(y_test, y_pred_lr)\n",
    "f1_lr = f1_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_lr}\")\n",
    "print(f\"Precision: {precision_lr}\")\n",
    "print(f\"Recall: {recall_lr}\")\n",
    "print(f\"F1 Score: {f1_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hyperparams_lr = {'penalty': ['l1', 'l2'],\n",
    "                  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  'solver': ['liblinear', 'saga', 'lbfgs']}\n",
    "\n",
    "lr_gs = GridSearchCV(lr, hyperparams_lr, cv=5)\n",
    "lr_gs.fit(X_train, y_train)\n",
    "print(f\"Best hyperparameters: {lr_gs.best_params_}\")\n",
    "print(f\"Best score: {lr_gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr_opt = LogisticRegression(penalty='l1', C=1, random_state=42, solver='liblinear') \n",
    "lr_opt.fit(X_train, y_train)\n",
    "y_pred_lr_opt = lr_opt.predict(X_test)\n",
    "\n",
    "accuracy_lr_opt = accuracy_score(y_test, y_pred_lr_opt)\n",
    "precision_lr_opt = precision_score(y_test, y_pred_lr_opt)\n",
    "recall_lr_opt = recall_score(y_test, y_pred_lr_opt)\n",
    "f1_lr_opt = f1_score(y_test, y_pred_lr_opt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_lr_opt}\")\n",
    "print(f\"Precision: {precision_lr_opt}\")\n",
    "print(f\"Recall: {recall_lr_opt}\")\n",
    "print(f\"F1 Score: {f1_lr_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(lr_opt, open('../models/lr_opt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #FFC300\">D. Test with K-Nearest Neighbor</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_train)\n",
    "y_pred_knn = knn_clf.predict(X_test)\n",
    "\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "precision_knn = precision_score(y_test, y_pred_knn)\n",
    "recall_knn = recall_score(y_test, y_pred_knn)\n",
    "f1_knn = f1_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_knn}\")\n",
    "print(f\"Precision: {precision_knn}\")\n",
    "print(f\"Recall: {recall_knn}\")\n",
    "print(f\"F1 Score: {f1_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_knn = {'n_neighbors': [3, 5, 7, 9, 11],\n",
    "                    'weights': ['uniform', 'distance'],\n",
    "                    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                    'p': [1, 2]}\n",
    "\n",
    "knn_gs = GridSearchCV(knn_clf, hyperparams_knn, cv=5)\n",
    "knn_gs.fit(X_train, y_train)\n",
    "print(f\"Best hyperparameters: {knn_gs.best_params_}\")\n",
    "print(f\"Best score: {knn_gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "knn_clf_opt = KNeighborsClassifier(n_neighbors=11, weights='uniform', algorithm='brute', p=1)\n",
    "knn_clf_opt.fit(X_train, y_train)\n",
    "y_pred_knn_opt = knn_clf_opt.predict(X_test)\n",
    "\n",
    "accuracy_knn_opt = accuracy_score(y_test, y_pred_knn_opt)\n",
    "precision_knn_opt = precision_score(y_test, y_pred_knn_opt)\n",
    "recall_knn_opt = recall_score(y_test, y_pred_knn_opt)\n",
    "f1_knn_opt = f1_score(y_test, y_pred_knn_opt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_knn_opt}\")\n",
    "print(f\"Precision: {precision_knn_opt}\")\n",
    "print(f\"Recall: {recall_knn_opt}\")\n",
    "print(f\"F1 Score: {f1_knn_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(knn_clf_opt, open('../models/knn_clf_opt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #FFC300\">E. Test with Naïve Bayes</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "y_pred_nb = nb_clf.predict(X_test)\n",
    "\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "precision_nb = precision_score(y_test, y_pred_nb)\n",
    "recall_nb = recall_score(y_test, y_pred_nb)\n",
    "f1_nb = f1_score(y_test, y_pred_nb)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_nb}\")\n",
    "print(f\"Precision: {precision_nb}\")\n",
    "print(f\"Recall: {recall_nb}\")\n",
    "print(f\"F1 Score: {f1_nb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
    "nb_gs = GridSearchCV(nb_clf, param_grid, cv=5)\n",
    "nb_gs.fit(X_train, y_train)\n",
    "print(f\"Best hyperparameters: {nb_gs.best_params_}\")\n",
    "print(f\"Best score: {nb_gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nb_clf_opt = GaussianNB(var_smoothing=1e-9)\n",
    "nb_clf_opt.fit(X_train, y_train)\n",
    "y_pred_nb_opt = nb_clf_opt.predict(X_test)\n",
    "\n",
    "accuracy_nb_opt = accuracy_score(y_test, y_pred_nb_opt)\n",
    "precision_nb_opt = precision_score(y_test, y_pred_nb_opt)\n",
    "recall_nb_opt = recall_score(y_test, y_pred_nb_opt)\n",
    "f1_nb_opt = f1_score(y_test, y_pred_nb_opt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_nb_opt}\")\n",
    "print(f\"Precision: {precision_nb_opt}\")\n",
    "print(f\"Recall: {recall_nb_opt}\")\n",
    "print(f\"F1 Score: {f1_nb_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(nb_clf_opt, open('../models/nb_clf_opt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #FFC300\">F. Test with Support Vector Machine</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svc = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "y_pred_svc = svc.predict(X_test_scaled)\n",
    "\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "precision_svc = precision_score(y_test, y_pred_svc)\n",
    "recall_svc = recall_score(y_test, y_pred_svc)\n",
    "f1_svc = f1_score(y_test, y_pred_svc)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_svc}\")\n",
    "print(f\"Precision: {precision_svc}\")\n",
    "print(f\"Recall: {recall_svc}\")\n",
    "print(f\"F1 Score: {f1_svc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid_svc = {'C': [0.1, 1, 10, 100],\n",
    "                  'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                  'gamma': ['scale', 'auto', 0.1, 1]}\n",
    "\n",
    "svc_gs = GridSearchCV(svc, param_grid_svc, cv=5)\n",
    "svc_gs.fit(X_train_scaled, y_train)\n",
    "print(f\"Best hyperparameters: {svc_gs.best_params_}\")\n",
    "print(f\"Best score: {svc_gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "svc_opt = SVC(C=100, kernel='linear', gamma='scale', random_state=42)\n",
    "svc_opt.fit(X_train_scaled, y_train)\n",
    "y_pred_svc_opt = svc_opt.predict(X_test_scaled)\n",
    "\n",
    "accuracy_svc_opt = accuracy_score(y_test, y_pred_svc_opt)\n",
    "precision_svc_opt = precision_score(y_test, y_pred_svc_opt)\n",
    "recall_svc_opt = recall_score(y_test, y_pred_svc_opt)\n",
    "f1_svc_opt = f1_score(y_test, y_pred_svc_opt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_svc_opt}\")\n",
    "print(f\"Precision: {precision_svc_opt}\")\n",
    "print(f\"Recall: {recall_svc_opt}\")\n",
    "print(f\"F1 Score: {f1_svc_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(svc_opt, open('../models/svc_opt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #FFC300\">G. Test with AdaBoost Classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=30,\n",
    "                             learning_rate=0.5, random_state=42)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "precision_ada = precision_score(y_test, y_pred_ada)\n",
    "recall_ada = recall_score(y_test, y_pred_ada)\n",
    "f1_ada = f1_score(y_test, y_pred_ada)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_ada}\")\n",
    "print(f\"Precision: {precision_ada}\")\n",
    "print(f\"Recall: {recall_ada}\")\n",
    "print(f\"F1 Score: {f1_ada}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid_ada = {'n_estimators': [50, 100, 150],\n",
    "                  'learning_rate': [0.1, 0.5, 1.0, 1.5],\n",
    "                  'algorithm': ['SAMME']}\n",
    "\n",
    "ada_gs = GridSearchCV(ada_clf, param_grid_ada, cv=5)\n",
    "ada_gs.fit(X_train, y_train)\n",
    "print(f\"Best hyperparameters: {ada_gs.best_params_}\")\n",
    "print(f\"Best score: {ada_gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ada_clf_opt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=150,\n",
    "                                    learning_rate=1.5, algorithm='SAMME', random_state=42)\n",
    "\n",
    "ada_clf_opt.fit(X_train, y_train)\n",
    "y_pred_ada_opt = ada_clf_opt.predict(X_test)\n",
    "\n",
    "accuracy_ada_opt = accuracy_score(y_test, y_pred_ada_opt)\n",
    "precision_ada_opt = precision_score(y_test, y_pred_ada_opt)\n",
    "recall_ada_opt = recall_score(y_test, y_pred_ada_opt)\n",
    "f1_ada_opt = f1_score(y_test, y_pred_ada_opt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_ada_opt}\")\n",
    "print(f\"Precision: {precision_ada_opt}\")\n",
    "print(f\"Recall: {recall_ada_opt}\")\n",
    "print(f\"F1 Score: {f1_ada_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_clf_opt, open('../models/ada_clf_opt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color: #FFC300\">H. Test with XGBoost Classifier</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=42)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_xgb}\")\n",
    "print(f\"Precision: {precision_xgb}\")\n",
    "print(f\"Recall: {recall_xgb}\")\n",
    "print(f\"F1 Score: {f1_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "param_grid_xgb = {'objective': ['binary:logistic', 'binary:logitraw', 'binary:hinge'],\n",
    "                  'max_depth': [3, 5, 7, 9],\n",
    "                  'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "                  'subsample': [0.5, 0.75, 1.0],\n",
    "                  'n_estimators': [50, 100, 150, 200]}  \n",
    "\n",
    "xgb_gs = GridSearchCV(xgb_clf, param_grid_xgb, cv=5)\n",
    "xgb_gs.fit(X_train, y_train)\n",
    "print(f\"Best hyperparameters: {xgb_gs.best_params_}\")\n",
    "print(f\"Best score: {xgb_gs.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgb_clf_opt = XGBClassifier(objective='binary:logistic', max_depth=3, learning_rate=0.5, subsample=0.75, n_estimators=100, random_state=42)\n",
    "xgb_clf_opt.fit(X_train, y_train)\n",
    "y_pred_xgb_opt = xgb_clf_opt.predict(X_test)\n",
    "\n",
    "accuracy_xgb_opt = accuracy_score(y_test, y_pred_xgb_opt)\n",
    "precision_xgb_opt = precision_score(y_test, y_pred_xgb_opt)\n",
    "recall_xgb_opt = recall_score(y_test, y_pred_xgb_opt)\n",
    "f1_xgb_opt = f1_score(y_test, y_pred_xgb_opt)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_xgb_opt}\")\n",
    "print(f\"Precision: {precision_xgb_opt}\")\n",
    "print(f\"Recall: {recall_xgb_opt}\")\n",
    "print(f\"F1 Score: {f1_xgb_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(xgb_clf_opt, open('../models/xgb_clf_opt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metric scores for each model in a dictionary\n",
    "metric_scores = {'Model': ['Decision Tree', 'Random Forest', 'Logistic Regression', 'K-Nearest Neighbors', 'Naive Bayes', 'Support Vector Machine', 'AdaBoost', 'XGBoost'],\n",
    "                 'Accuracy': [accuracy_opt, accuracy_rf_opt, accuracy_lr_opt, accuracy_knn_opt, accuracy_nb_opt, accuracy_svc_opt, accuracy_ada_opt, accuracy_xgb_opt],\n",
    "                 'Precision': [precision_opt, precision_rf_opt, precision_lr_opt, precision_knn_opt, precision_nb_opt, precision_svc_opt, precision_ada_opt, precision_xgb_opt],\n",
    "                 'Recall': [recall_opt, recall_rf_opt, recall_lr_opt, recall_knn_opt, recall_nb_opt, recall_svc_opt, recall_ada_opt, recall_xgb_opt],\n",
    "                 'F1 Score': [f1_opt, f1_rf_opt, f1_lr_opt, f1_knn_opt, f1_nb_opt, f1_svc_opt, f1_ada_opt, f1_xgb_opt]}\n",
    "\n",
    "df_metric_scores = pd.DataFrame(metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metric scores to a CSV file\n",
    "df_metric_scores.to_csv('../data/processed/metric_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
